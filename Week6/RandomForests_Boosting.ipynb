{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPFnEVO-Xoiw",
        "outputId": "b04241da-2c08-4485-8ea5-9c83d6b216a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training observations: 1400\n",
            "Test observations:      600\n",
            "\n",
            "======================================================================\n",
            "RANDOM FOREST CLASSIFIER\n",
            "======================================================================\n",
            "\n",
            "Training accuracy:  0.969\n",
            "Test accuracy:       0.917\n",
            "\n",
            "Classification report (test set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  No default       0.93      0.96      0.94       418\n",
            "     Default       0.89      0.82      0.86       182\n",
            "\n",
            "    accuracy                           0.92       600\n",
            "   macro avg       0.91      0.89      0.90       600\n",
            "weighted avg       0.92      0.92      0.92       600\n",
            "\n",
            "Confusion matrix (test set):\n",
            "[[400  18]\n",
            " [ 32 150]]\n",
            "\n",
            "Random Forest feature importances (higher = more important):\n",
            "              feature  importance\n",
            "       num_past_loans    0.260945\n",
            "  balance_outstanding    0.238456\n",
            "payment_history_score    0.230637\n",
            "               income    0.205016\n",
            "                  age    0.033159\n",
            "  regional_risk_index    0.031788\n",
            "\n",
            "======================================================================\n",
            "GRADIENT BOOSTING CLASSIFIER\n",
            "======================================================================\n",
            "\n",
            "Training accuracy:  0.966\n",
            "Test accuracy:       0.918\n",
            "\n",
            "Classification report (test set):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  No default       0.93      0.95      0.94       418\n",
            "     Default       0.88      0.84      0.86       182\n",
            "\n",
            "    accuracy                           0.92       600\n",
            "   macro avg       0.91      0.90      0.90       600\n",
            "weighted avg       0.92      0.92      0.92       600\n",
            "\n",
            "Confusion matrix (test set):\n",
            "[[398  20]\n",
            " [ 29 153]]\n",
            "\n",
            "Gradient Boosting feature importances (higher = more important):\n",
            "              feature  importance\n",
            "  balance_outstanding    0.291677\n",
            "payment_history_score    0.236743\n",
            "               income    0.228070\n",
            "       num_past_loans    0.223412\n",
            "  regional_risk_index    0.012426\n",
            "                  age    0.007671\n",
            "\n",
            "======================================================================\n",
            "RANDOM FOREST WITH GRID SEARCH (CROSS-VALIDATION)\n",
            "======================================================================\n",
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "\n",
            "Best parameters from GridSearchCV:\n",
            "{'max_depth': None, 'max_features': 0.7, 'n_estimators': 200}\n",
            "\n",
            "Best cross-validated accuracy: 0.926\n",
            "\n",
            "Performance of tuned Random Forest on test set:\n",
            "Test accuracy: 0.925\n",
            "Confusion matrix:\n",
            "[[398  20]\n",
            " [ 25 157]]\n",
            "\n",
            "======================================================================\n",
            "GRADIENT BOOSTING WITH GRID SEARCH (CROSS-VALIDATION)\n",
            "======================================================================\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "\n",
            "Best parameters from GB GridSearchCV:\n",
            "{'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}\n",
            "Best cross-validated accuracy: 0.912\n",
            "\n",
            "Performance of tuned Gradient Boosting on test set:\n",
            "Test accuracy: 0.920\n",
            "Confusion matrix:\n",
            "[[398  20]\n",
            " [ 28 154]]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Solution to class assignment\n",
        "trees_ensembles_econ.py\n",
        "\n",
        "Teaching example for Random Forests and Gradient Boosting using scikit-learn.\n",
        "Context: Master's students in economics learning applied ML.\n",
        "\n",
        "We build a synthetic default-risk dataset with economic-style covariates,\n",
        "fit RandomForest and GradientBoosting models, evaluate them, and look at\n",
        "feature importances and simple hyperparameter tuning.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "\n",
        "def make_dataset(n_samples=2000, random_state=0):\n",
        "    \"\"\"\n",
        "    Create a synthetic binary classification dataset that resembles\n",
        "    an economics application: predicting loan default.\n",
        "    \"\"\"\n",
        "    X, y = make_classification(\n",
        "        n_samples=n_samples,\n",
        "        n_features=6,\n",
        "        n_informative=4,\n",
        "        n_redundant=0,\n",
        "        n_repeated=0,\n",
        "        n_clusters_per_class=2,\n",
        "        weights=[0.7, 0.3],   # 30% default rate\n",
        "        class_sep=1.2,\n",
        "        random_state=random_state,\n",
        "    )\n",
        "\n",
        "    # Give features economic-style names\n",
        "    feature_names = [\n",
        "        \"income\",            # roughly: higher => less default\n",
        "        \"age\",\n",
        "        \"balance_outstanding\",\n",
        "        \"num_past_loans\",\n",
        "        \"payment_history_score\",\n",
        "        \"regional_risk_index\",\n",
        "    ]\n",
        "\n",
        "    df = pd.DataFrame(X, columns=feature_names)\n",
        "    df[\"default\"] = y\n",
        "    return df, feature_names\n",
        "\n",
        "\n",
        "def train_random_forest(X_train, y_train, X_test, y_test, feature_names):\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"RANDOM FOREST CLASSIFIER\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # A reasonably standard RF setup\n",
        "    rf = RandomForestClassifier(\n",
        "        n_estimators=200,      # number of trees\n",
        "        max_depth=None,        # let trees grow deep\n",
        "        max_features=\"sqrt\",   # random subset of features at each split\n",
        "        min_samples_leaf=5,    # regularization\n",
        "        random_state=0,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_train = rf.predict(X_train)\n",
        "    y_pred_test = rf.predict(X_test)\n",
        "\n",
        "    print(\"\\nTraining accuracy:  {:.3f}\".format(accuracy_score(y_train, y_pred_train)))\n",
        "    print(\"Test accuracy:       {:.3f}\".format(accuracy_score(y_test, y_pred_test)))\n",
        "\n",
        "    print(\"\\nClassification report (test set):\")\n",
        "    print(classification_report(y_test, y_pred_test, target_names=[\"No default\", \"Default\"]))\n",
        "\n",
        "    print(\"Confusion matrix (test set):\")\n",
        "    print(confusion_matrix(y_test, y_pred_test))\n",
        "\n",
        "    # Feature importances\n",
        "    importances = rf.feature_importances_\n",
        "    importance_df = pd.DataFrame(\n",
        "        {\"feature\": feature_names, \"importance\": importances}\n",
        "    ).sort_values(\"importance\", ascending=False)\n",
        "\n",
        "    print(\"\\nRandom Forest feature importances (higher = more important):\")\n",
        "    print(importance_df.to_string(index=False))\n",
        "\n",
        "\n",
        "def train_gradient_boosting(X_train, y_train, X_test, y_test, feature_names):\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"GRADIENT BOOSTING CLASSIFIER\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # A simple GBM; learning_rate and n_estimators are the key knobs\n",
        "    gb = GradientBoostingClassifier(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.05,\n",
        "        max_depth=3,     # depth of individual trees (weak learners)\n",
        "        subsample=0.8,   # stochastic gradient boosting\n",
        "        random_state=0,\n",
        "    )\n",
        "\n",
        "    gb.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_train = gb.predict(X_train)\n",
        "    y_pred_test = gb.predict(X_test)\n",
        "\n",
        "    print(\"\\nTraining accuracy:  {:.3f}\".format(accuracy_score(y_train, y_pred_train)))\n",
        "    print(\"Test accuracy:       {:.3f}\".format(accuracy_score(y_test, y_pred_test)))\n",
        "\n",
        "    print(\"\\nClassification report (test set):\")\n",
        "    print(classification_report(y_test, y_pred_test, target_names=[\"No default\", \"Default\"]))\n",
        "\n",
        "    print(\"Confusion matrix (test set):\")\n",
        "    print(confusion_matrix(y_test, y_pred_test))\n",
        "\n",
        "    # Feature importances\n",
        "    importances = gb.feature_importances_\n",
        "    importance_df = pd.DataFrame(\n",
        "        {\"feature\": feature_names, \"importance\": importances}\n",
        "    ).sort_values(\"importance\", ascending=False)\n",
        "\n",
        "    print(\"\\nGradient Boosting feature importances (higher = more important):\")\n",
        "    print(importance_df.to_string(index=False))\n",
        "\n",
        "\n",
        "def random_forest_with_cv(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Simple example of using cross-validation to choose RF hyperparameters.\n",
        "    This is just for teaching; the grid is tiny.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"RANDOM FOREST WITH GRID SEARCH (CROSS-VALIDATION)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    rf = RandomForestClassifier(random_state=0, n_jobs=-1)\n",
        "\n",
        "    param_grid = {\n",
        "        \"n_estimators\": [100, 200],\n",
        "        \"max_depth\": [None, 5, 10],\n",
        "        \"max_features\": [\"sqrt\", 0.7],\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=rf,\n",
        "        param_grid=param_grid,\n",
        "        cv=5,              # 5-fold cross-validation\n",
        "        scoring=\"accuracy\",\n",
        "        n_jobs=-1,\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(\"\\nBest parameters from GridSearchCV:\")\n",
        "    print(grid_search.best_params_)\n",
        "\n",
        "    print(\"\\nBest cross-validated accuracy: {:.3f}\".format(grid_search.best_score_))\n",
        "\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "\n",
        "\n",
        "def gradient_boosting_with_cv(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Example of using cross-validation to choose Gradient Boosting hyperparameters.\n",
        "    Again, small grid for teaching purposes.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"GRADIENT BOOSTING WITH GRID SEARCH (CROSS-VALIDATION)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    gb = GradientBoostingClassifier(random_state=0)\n",
        "\n",
        "    # Key knobs: learning_rate, n_estimators, max_depth, subsample\n",
        "    param_grid = {\n",
        "        \"n_estimators\": [100, 200],\n",
        "        \"learning_rate\": [0.05, 0.1],\n",
        "        \"max_depth\": [2, 3],\n",
        "        \"subsample\": [0.8, 1.0],\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=gb,\n",
        "        param_grid=param_grid,\n",
        "        cv=5,\n",
        "        scoring=\"accuracy\",\n",
        "        n_jobs=-1,\n",
        "        verbose=1,\n",
        "    )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(\"\\nBest parameters from GB GridSearchCV:\")\n",
        "    print(grid_search.best_params_)\n",
        "\n",
        "    print(\"Best cross-validated accuracy: {:.3f}\".format(grid_search.best_score_))\n",
        "\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 1. Create dataset\n",
        "    df, feature_names = make_dataset()\n",
        "    X = df[feature_names].values\n",
        "    y = df[\"default\"].values\n",
        "\n",
        "    # 2. Train-test split (hold-out set mimics out-of-sample prediction)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=0, stratify=y\n",
        "    )\n",
        "\n",
        "    print(\"Training observations:\", X_train.shape[0])\n",
        "    print(\"Test observations:     \", X_test.shape[0])\n",
        "\n",
        "    # 3. Fit and evaluate Random Forest\n",
        "    train_random_forest(X_train, y_train, X_test, y_test, feature_names)\n",
        "\n",
        "    # 4. Fit and evaluate Gradient Boosting\n",
        "    train_gradient_boosting(X_train, y_train, X_test, y_test, feature_names)\n",
        "\n",
        "    # Using Cross-validation to select hyperparameters (Random Forest)\n",
        "    best_rf = random_forest_with_cv(X_train, y_train)\n",
        "\n",
        "    # 5. Evaluate tuned RF on test set\n",
        "    y_pred_test_best = best_rf.predict(X_test)\n",
        "    print(\"\\nPerformance of tuned Random Forest on test set:\")\n",
        "    print(\"Test accuracy: {:.3f}\".format(accuracy_score(y_test, y_pred_test_best)))\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred_test_best))\n",
        "\n",
        "    # 6. GB with CV\n",
        "    best_gb = gradient_boosting_with_cv(X_train, y_train)\n",
        "    y_pred_test_best_gb = best_gb.predict(X_test)\n",
        "    print(\"\\nPerformance of tuned Gradient Boosting on test set:\")\n",
        "    print(\"Test accuracy: {:.3f}\".format(accuracy_score(y_test, y_pred_test_best_gb)))\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred_test_best_gb))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aJRYz7qrXp0G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}